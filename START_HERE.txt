â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    DATASET CRAWLER - C++ PROJECT                          â•‘
â•‘           Web Crawler with Parquet Export for AI Training Data             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ WHAT IS THIS PROJECT?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

A complete C++ web crawler application that collects web data and saves it in 
Parquet format (optimized for machine learning and data science).

âœ¨ KEY FEATURES:
  âœ“ Multi-threaded web crawling
  âœ“ Parquet format export (5-10x smaller than CSV)
  âœ“ CSV export for compatibility
  âœ“ Configurable timeouts and headers
  âœ“ Full error handling
  âœ“ Docker support included
  âœ“ Python utilities for data analysis
  âœ“ Complete documentation

âš¡ QUICK START (3 STEPS):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. INSTALL DEPENDENCIES:
   $ ./install_dependencies.sh

2. BUILD THE PROJECT:
   $ make build

3. RUN THE CRAWLER:
   $ ./build/crawler

Output: dataset.parquet + dataset.csv

ğŸ“š DOCUMENTATION:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

START HERE:     QUICKSTART.md      (Read this first - 5 min read)
Full Docs:      README.md          (Complete API reference)
Developer:      DEVELOPER.md       (Architecture & extending)
Project Info:   INDEX.md           (File listing & structure)
Quick Ref:      CHEATSHEET.md      (Command shortcuts)

ğŸ”§ WHAT'S INCLUDED:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

C++ Code:
  â€¢ WebCrawler      - Single-threaded HTTP client
  â€¢ AdvancedCrawler - Multi-threaded crawler with thread pool
  â€¢ ParquetWriter   - Parquet file export
  â€¢ 6 working examples

Build System:
  â€¢ CMakeLists.txt  - CMake configuration
  â€¢ Makefile        - Convenient build commands
  â€¢ Dockerfile      - Container image
  â€¢ Docker Compose  - Orchestration

Python Tools:
  â€¢ parquet_utils.py - Data analysis and format conversion

ğŸ“¦ DEPENDENCIES (AUTO-INSTALLED):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

System:
  â€¢ CMake 3.20+
  â€¢ C++17 compiler (GCC/Clang)
  â€¢ libcurl4-openssl-dev
  â€¢ libparquet-dev
  â€¢ libparquet0

Python:
  â€¢ pandas
  â€¢ pyarrow
  â€¢ numpy

Optional:
  â€¢ Docker & Docker Compose

ğŸ’¡ USAGE EXAMPLES:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

COMMAND LINE:
  $ ./build/crawler                     # Basic crawl
  $ make run                            # Full build + run
  $ docker-compose up                   # Docker mode

PYTHON ANALYSIS:
  $ python3 scripts/parquet_utils.py info dataset.parquet
  $ python3 scripts/parquet_utils.py to-csv dataset.parquet out.csv
  $ python3 scripts/parquet_utils.py merge part*.parquet -o final.parquet

C++ CODE:
  #include "crawler.h"
  WebCrawler crawler;
  auto records = crawler.crawl_urls(urls);
  
  ParquetDatasetWriter writer;
  writer.write_records("output.parquet", records);

ğŸ“Š PROJECT STATISTICS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Total Files:        56
C++ Code Lines:     808
Documentation:      4 files (~40 KB)
Total Size:         372 KB
Build Time:         ~10 seconds

ğŸš€ NEXT STEPS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Read QUICKSTART.md (best way to get started)
2. Install: ./install_dependencies.sh
3. Build: make build
4. Run: ./run.sh run
5. Analyze: python3 scripts/parquet_utils.py info dataset.parquet
6. Customize: Edit config.json and urls.txt for your data

â“ HELP & SUPPORT:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Documentation:   Read QUICKSTART.md or README.md
Examples:        Check examples/examples.cpp
Commands:        Run './run.sh help' or 'make help'
Architecture:    See DEVELOPER.md for internals
Troubleshooting: Check QUICKSTART.md "Ğ Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼" section

ğŸ“œ LICENSE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MIT License - Free for personal and commercial use
See LICENSE file for details

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ready to go! Open QUICKSTART.md to begin.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
